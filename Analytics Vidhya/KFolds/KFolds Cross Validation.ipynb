{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "Source: http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\n",
    "## Basic cross-validation\n",
    "Cross validation is a method of splitting your data set into chunks (folds) and using one part as a test/validation set and the rest as a training set. You can create, fit and score the model using different combinations of the data folds as test and train sets and take an average of the results to get a more accurate score for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, svm # import sample data and svm module\n",
    "\n",
    "digits = datasets.load_digits() # load sample data\n",
    "x_features = digits.data # features/predictors\n",
    "y_labels = digits.target # labels/outcomes\n",
    "\n",
    "svc = svm.SVC(C=1, kernel='linear') # define model as linear svm\n",
    "svc.fit(x_features[:-100], y_labels[0:-100]).score(x_features[-100:], y_labels[-100:]) # fit model and calculate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Folds\n",
    "The above model simply builds and applies the model once to the entire dataset, using one part to fit and one part to test its accuracy. This provides an accuracy of just under 98% but this is a one off run and we can use KFolds to get a more accurate and reliable accuracy prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# split x and y into 3 equal folds\n",
    "x_folds = np.array_split(x_features, 3)\n",
    "y_folds = np.array_split(y_labels, 3)\n",
    "scores = list()\n",
    "\n",
    "# iterate through folds\n",
    "for k in range(3):\n",
    "    x_train = list(x_folds) # create copy of folds\n",
    "    x_test = x_train.pop(k) # extract fold for test\n",
    "    x_train = np.concatenate(x_train) # use remaining folds for train\n",
    "    y_train = list(y_folds) # same for y/outcomes\n",
    "    y_test = y_train.pop(k)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    \n",
    "    scores.append(svc.fit(x_train, y_train).score(x_test, y_test)) # record each score\n",
    "\n",
    "print(scores) # print all scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Folds and Generators/Customisation\n",
    "We can use more complex generators to split the data in custom ways and then analyse these methods. This split function attached to the kfold model splits your dataset into train and test data, you can define how many times you test the data and how many folds/samples you split the data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5] | Test: [0 1]\n",
      "Train: [0 1 4 5] | Test: [2 3]\n",
      "Train: [0 1 2 3] | Test: [4 5]\n",
      "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "x = ['a', 'a', 'b', 'c', 'c', 'c'] # sample size is 6 (split method below splits data into 6 samples, using 4 as train and 2 as test)\n",
    "k_fold = KFold(n_splits=3) # 3 equal folds\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(x): # splits the data into train and test based on the x variable above\n",
    "    print('Train: %s | Test: %s' % (train_indices, test_indices)) # show train, test data for each fold\n",
    "\n",
    "# print scores \n",
    "print([svc.fit(x_features[train], y_labels[train]).score(x_features[test], y_labels[test]) for train, test in k_fold.split(x_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_val_score\n",
    "This helper function computes all of the inputs of your process, including your estimator (svm/svc here), cross-validation object (KFolds here) and input data set. It then splits your dataset repeatedly, applies the estimator to each training set and validates it against each test set to produce an array of individual scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93489149,  0.95659432,  0.93989983])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_jobs=-1 tells it to use all CPUs on your PC\n",
    "cross_val_score(svc, x_features, y_labels, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this outputs the same results as the previous code, essentially this helper offers a shorthand toolkit for plugging in all of the various objects and models you need to run your cross-validation, saving you time and code.\n",
    "\n",
    "By default, the accuracy scores are determined using the estimator that you selected and its 'scoring' method. You can manually select alternate scoring methods if you wish to have a specific analysis of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93969761,  0.95911415,  0.94041254])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, x_features, y_labels, cv=k_fold, scoring='precision_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific 'precision_macro' scoring method produces different results. A list of different scoring methods and their meaning can be found in the following link: http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "As well as the standard KFolds method which we have been using, there are many other methods of cross validation, these can be found in the following link: http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\n",
    "\n",
    "## Grid Search\n",
    "This is a method of plugging in an estimator (e.g. svc) and the dataset and then the grid search tool creates a grid of parameters and calculates the best combination of parameters to maximise the cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | Best score: 0.925 | Best estimator: 0.00774263682681\n",
      "TRAIN | Score: 0.943538268507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "params = np.logspace(-6, -1, 10) # creates values along a log scale (start, stop, sample)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=params), n_jobs=-1)\n",
    "\n",
    "clf.fit(x_features[:1000], y_labels[:1000])\n",
    "print('TEST | Best score: %s | Best estimator: %s' % (clf.best_score_, clf.best_estimator_.C))\n",
    "print('TRAIN | Score: %s' % (clf.score(x_features[1000:], y_labels[1000:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the performance is better on the train data rather than the test.\n",
    "\n",
    "By default GridSearchCV uses a 3-fold cross-validation, however if it detects a classifier (e.g. SVM) instead of a regressor it uses a stratified 3-fold. Stratification is where the data is rearranged when splitting so that each fold is representative of the sample (e.g. if there are 2 folds and 2 classes represented, it aims to have each class as ~50% in each fold), this improves bias and variance in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93853821,  0.96327212,  0.94463087])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, x_features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are performing nested cross-validation. The cross_val_score is estimating the prediction score of the 3 folds and within each of these folds the GridSearchCV is estimating the best parameters to use.\n",
    "\n",
    "Therefore the resulting scores are unbiased estimates of the prediction accuracy when applied to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "* Finish off this source, there is one final section on cross-validated estimators being automatically calculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
